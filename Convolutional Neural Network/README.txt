In model.py file, there four different models. First model uses convolution operation and activation function for each layer. The first model uses 8 (5x5x3) filters with stride 1 and padding 2 in first layer and ReLu activation function, second layer has 16 (5x5x8) filters and ReLu activation function, third convolution layer has 32 (5x5x16) filters and ReLu activation function and there is one fully connected layer at the end. The second model is to compare what kind of result we will get when we increase the number of fully connected layers. It is basically the same as the first model, only the 2nd model has 3 fully connected layers. Also, third model use first model as base model. The third model additionally has dropouts between layers. Similarly, forth model has batch normalization between layers.

In main.py file, we train the models with SGD optimizer and 0.01 learning rate. The batch size is selected as 100 and the number of epochs is 20. Here, we use Bjarte Mehus Sundeâ€™s GitHub repository (2019) as early stopping procedure. In this repository, there is a file which name is pytorchtools.py. Increase in validation loss can say overfitting problem in the model. He says the model should stop the training if the validation loss starts to increase. We clone that file and we use it in model training. 

In eval.py file, we try to train model 4 with different optimizers ( Adam optimizer and Adagrad optimizer). After we training these models, we get the model with SGD optimizer. 

In tsne.py file, we plot tsne function by training model 4. In the beginning of the training, it is difficult to classify the groups clearly. This plot is after only 1 epochs. Here, we use model 4 to get tsne plot and the model hyperparameters are the same. Because we know the forth model stops at 4th epochs, we train it with 4 epochs. Therefore, we get tsne plot for each epoch. After run this file, we get one plot for each epoch and therefore, we obtain 4 different tsne plot since we train model for 4 epochs. 

In this project, we use CIFAR10 dataset. Data can be downloaded from https://drive.google.com/file/d/1w5vVDMtBOO7A6bB7DYWg9-TUwzWvU2MI/view?usp=sharing 