
In this project, we will implement a VAE for MNIST dataset. MNIST dataset includes grayscale(that is, images have only one color channel) 28x28 images of handwritten digits. We try to generate new data by using VAE architecture. VAE enables to get a latent space by using the original data and thus, we obtain a sample. In the encoding part, we produce mean and variance values. We decode the sample according to these mean and variance values and we get new data.


In model.py file, we have Variational Auto-encoder model. The model has LSTM as encoder and transpose convolution layers as decoder. In pytorch, lstm layer gives us a tensor including hidden state, a tensor including cell or internal state and output features. Also, in decoder part, we use transpose convolution layers because we generate new data from random vectors containing the outputs of encoding part. For a single LSTM layer, we use 32 as dimension of hidden state. We get 28x32 dimensional output. After the LSTM layer, we use ReLU activation function and fully connected layer with 32x256 dimensional linear vectors. Then, we calculate mean and variance of the latent space and create a sample by using them. In the CNN decoder part, we use 3 transpose convolutional layers. The transpose convolutional layers contain transpose convolution operation, ReLU activation function, batch normalization, pooling and dropout. At the end of the decoder part, we should 1x28x28 images. (”1” says they are gray-scale images.) Also, model.py file has loss function. We use binary cross entropy loss and KL divergence for regularization term. 


In main.py file, we upload the MNIST data using pytorch. At first, we visualize images before model training to compare results. For training the model, we can use google colaboratory to use GPU. Therefore, the model training spent about only 5 minutes. Our learning rate is 0.001 and Adam optimizer is selected. Also, batch size is 100 and number of epochs is 50. In main.py file, we normalize data before starting to model training. In the beginning of the model training, our loss is 0.2699808180332184 and regularization term with KL divergence is 0.01405326183885336 while the loss after 50 epochs is 0.1622491180896759 and the regularization term is 0.021074645221233368. During the training, we save reconstruction images as numpy arrays and we concatenate them at the end of the training. We have 60000 reconstruction images after the VAE model run during 50 epochs. The generated images belong to fiftieth epoch. At the beginning of the training, the generated images are not starting to form fully because the model has not learned enough yet. That’s why we save only the results at the end of training. 


In model.pk file, we have the results in main.py file. It contains, 60000 generated vectors after decoder part, training losses and regularization terms for 50 epochs. We will use this file for the visualization of images. Also, we can use it for change of loss and regularization term of the model.


In generator.py file, we have generated images from randomly selected from 100 vectors after decoder part. We have 60000 reconstruction images after the VAE model run during 50 epochs. The generated images belong to fiftieth epoch. We select 100 vectors randomly by using pytorch and visualize them with matplotlib library. 

In pdf file for the report of the project, we can find VAE model architecture in detail and its results. It contains model outputs shapes, effect of KL divergence regularization term, the loss and regularization term change plots during 50 epochs and finally, reconstruction images. 

