{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /Users/elifyilmaz/opt/anaconda3/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: six in /Users/elifyilmaz/opt/anaconda3/lib/python3.8/site-packages (from h5py) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/elifyilmaz/opt/anaconda3/lib/python3.8/site-packages (from h5py) (1.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/elifyilmaz/opt/anaconda3/lib/python3.8/site-packages (4.54.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here, first of all I tried it for given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can anyone else have a stranger become home and games answer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "\n",
    "textgen = textgenrnn()\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,000 texts collected.\n",
      "Training on 83,501 character sequences.\n",
      "652/652 [==============================] - ETA: 0s - loss: 1.7692####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "Google company is a startup of a startup\n",
      "\n",
      "A company is not starting on the startup of the startup\n",
      "\n",
      "A startup of the Universe of Startup\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I think I want to an and show sential end of servers\n",
      "\n",
      "What a simple source controvern swat\n",
      "\n",
      "Good for a data and locker to let your state transportation old\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Announces explicate car from 311 felting 9115 loop is not just end on Sony to Gotbap Use/I Ignogie\n",
      "\n",
      "Hellows $122\n",
      "\n",
      "Had YC (2010)\n",
      "\n",
      "652/652 [==============================] - 173s 265ms/step - loss: 1.7692\n",
      "How a standary of the survey startup developer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.train_from_file('datasets/hacker_news_2000.txt', num_epochs=1)\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, I found SMS Spam Collection in English data set from https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textgenrnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,574 texts collected.\n",
      "Training on 477,088 character sequences.\n",
      "Epoch 1/2\n",
      "3727/3727 [==============================] - ETA: 0s - loss: 1.7604####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "hamNo shopping and and send the way to get the stread of the man will be sorry to get the be and then you can get the shit of mate. I can get a little now will be a good is a good morning. I can get the stread and i want to stop a £2000 prize GUARANT to 8770666666 110p/more call 087110002000000 per\n",
      "\n",
      "hamGood message. I can get a good call of a good and start in a minutes of the mobile and i was then you have a good day. Thats you are trying to come to be a good teach and i send you to get the message. I can get the way to anyone and the little story to 8700 and send a £100 prize GUARANT to 8700\n",
      "\n",
      "hamHave you gonna get the man to me and start and the boy is a good teach and the new year day. Call 09066696666 and I will be one to your and i have a go to change the mates and watch that was the good month and i will be a good time. I can get that and i can get a come back of the man and i can g\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "hamNo and be a big time to you and send day. We have a £2000 cash and I will ready out and i can get the day. I am not going to get a happy and gonna be and text me a charter\n",
      "\n",
      "hamHi week and i have a guarantay and i got a go the mobile now to see me to be a good street to u get the one and chat to the way and arrive you to see. I need to start be you when you know I just call me and wat the way in the minute of sister has been you eveninging me and i had you goind to see\n",
      "\n",
      "hamYeah i have to get the stop i even he are going to get to me. I have to be a different the day and send take a love of the same and the half in a suns at the day. It's a mobile and all going to stop the teast to your medas on my chance of the way. Have a £2000 per minly and how are you to be a c\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "hamLanter pls start have all out\n",
      "\n",
      "hamCoga Has that neck? Unsijuy lucoh r i've come a 500 customers to the art of day in al \n",
      "\n",
      "spamNobilities on 806 likes in three is 1/monety now Free claim. Get  &lt;#&gt;  not text me.ust it to try rain if it's worries before  &lt;#&gt;  UK folm.\n",
      "\n",
      "3727/3727 [==============================] - 1049s 281ms/step - loss: 1.7604\n",
      "Epoch 2/2\n",
      "3727/3727 [==============================] - ETA: 0s - loss: 1.5809####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "hamHey a story to stop a story of the part of the car of friends and send you and the way the story of the sister that is the part of the same car or some people to contact you be starting to contact you and the start to the password is the first the store to the present and there is the start of m\n",
      "\n",
      "hamHey i want to be a like and there is the presentation of the part of the top super da the part of more prize. Call 0906505023 to 8706 to 8007. 150p/min call 090650000000 0871115000 150ppm\n",
      "\n",
      "hamHey i have a simple prize of your mobile to see you a bit to collect the train of the words of the story of the same watch to the latest then i was still in the pain to be a find of the sister to the time of the prize of your mobile to collect you and there is the presentation of the same servic\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "hamYou have to pay some people and a code to pay care. May u get us on ur free to do any computer cash or you and there is in the part at on foreial... I am took to do anything later\n",
      "\n",
      "hamSo a simple prize a year to send it to stay in my friends and them really dun  &lt;#&gt;  line. How make i remember me and how are you too house? I do me a day.\n",
      "\n",
      "hamHello you can i come in the drug mail. I hope you sent me a bit for the real holiday of day so i think i am hairing you get the money from a Meeting Q £500 free on 0800382236122. 3xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "hamOh\n",
      "\n",
      "spamALF feel much to prak you back! Oz Got my eyes for the macchmainit. New you what you come to go beg? It will get father? \"I wants the day to draw to get smiles and so thanks for few days.\n",
      "\n",
      "hamAnymay too mistains. Only 1 is from 2 with the first bonus freek condition. S. Nu winn...now ?.. Just GON. Why is The Cudderfu Swhee Hope 2 Replying Freemcoloves increase shorter incudentdCas landline in a redeeper! We am and gene meg ? Text mum. Pilable iis. HMISSLE 81, I HMV WAKE WANT UNIT UR \n",
      "\n",
      "3727/3727 [==============================] - 1076s 289ms/step - loss: 1.5809\n",
      "hamHi them is the best of you tonight ? You need to see a statem earlier who love you have a good day to do what you remember that you address to send you a story of the paper on me to complete you to stop there to the phone. I miss you be not haired that but it was stuffing here? I was on the pub \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train_from_file('smsspamcollection/SMSSpamCollection.txt', num_epochs=2)\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also, I found Nietzsche text data set from https://www.kaggle.com/pankrzysiu/nietzsche-texts but it is very long so I used first part of it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,930 texts collected.\n",
      "Training on 377,315 character sequences.\n",
      "2947/2947 [==============================] - ETA: 0s - loss: 1.5512####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "the only and present is the most problem of the hand and the problem of the strength and the strange and strange and strange and and as the most strength and the for the strange of the strength of the present of the strength of the strength of the more present of the more will an art of the because\n",
      "\n",
      "and and desire of the strange of the present of the most strange and still; which is always and and the problem of the most part of the present and end and the strange of the more strange of the more and strange and the strange of the problem and strange of the problem of the morality of the more a\n",
      "\n",
      "and strange and still; and the present of the present of the more strange and from the strength and the strength and strange of the present of the hand of the problem of the problem and strength of the more and the strength and strange of the more the strength of the problem of the hand of the beli\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "the strength and fair man of structions and man when he who has\n",
      "\n",
      "and still about the satisfee of the more constectate diveral strength present and the most democratic of the\n",
      "\n",
      "and the mead of a realize and have been still the spirit and as which the perhaps are to be as every strength the most said and present of more personal suble of the man is a subtle with a presunted and persist one seems to any other traive in the care for the part of a new; and it is a present tim\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "It enerfortu and democre of every card and him, stupy\n",
      "\n",
      "forcial questions, and old man who? Ass in\n",
      "\n",
      "that the origin. It is perhaps and \"ash.\" WAK\n",
      "\n",
      "2947/2947 [==============================] - 956s 324ms/step - loss: 1.5512\n",
      "or a strong that the belongs who are the worth and the present of the problem of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train_from_file('nietzsche copy.txt', num_epochs=1)\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, I used a data set by using scraping in class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    url = \"https://americanenglish.state.gov/files/ae/resource_files/the_tell-tale_heart_0.pdf\"\n",
    "    r = requests.get(url, stream=True)\n",
    "    \n",
    "    bar = tqdm (\n",
    "        total = int(r.headers[\"Content-Length\"]),\n",
    "        initial = 0,\n",
    "        unit = 'B',\n",
    "        unit_scale = True,\n",
    "    )\n",
    "    \n",
    "    with open(\"data.zip\", \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                bar.update(1024)\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_words():\n",
    "    words = []\n",
    "    with open(\"text.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            for word in line.strip().split(\" \"):\n",
    "                words.append(word)\n",
    "                \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(words, n_words):\n",
    "    freq = [['UNKNOWN', -1]]\n",
    "    \n",
    "    c = Counter(words)\n",
    "    pairs = list(c.items())\n",
    "    pairs_filtered = sorted(pairs, key=lambda x: -x[1])[:n_words-1] \n",
    "    for word, count in pairs_filtered: \n",
    "        freq.append([word, count])\n",
    "        \n",
    "    dictionary = dict()\n",
    "    for word, _ in freq:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    data = list()\n",
    "    unknown_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNKNOWN']\n",
    "            unknown_count += 1\n",
    "        data.append(index)\n",
    "        \n",
    "    freq[0][1] = unknown_count\n",
    "    \n",
    "    inverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, freq, dictionary, inverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162kB [00:00, 621kB/s]                            \n"
     ]
    }
   ],
   "source": [
    "download_data()\n",
    "words = fetch_words()\n",
    "\n",
    "vector_dim = 300\n",
    "epochs = 200000\n",
    "vocabulary_upper_limit = 10000\n",
    "data, freq, dictionary, inverse_dictionary = build_data(words, vocabulary_upper_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 texts collected.\n",
      "Training on 10,678 character sequences.\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6218####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I had passed a small character of my dead at the person of the night of the shades, there is no interpelant of the front upon and what the would a betweeve the opening of heards, nothing with the officient of the mistake and heard. I was nothered and had to lead the son. I dont thanking out of my h\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "\n",
      "\n",
      "While Build! A still, his doorpry. I need to shares, increased, dusted the before a statie I laughed.\n",
      "\n",
      "\n",
      "\n",
      "83/83 [==============================] - 30s 359ms/step - loss: 1.6218\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train_from_file('text.txt', num_epochs=1)\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocabulary_upper_limit)\n",
    "couples, labels = skipgrams(data, vocabulary_upper_limit, window_size=3, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1932"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_target, input_context = Input(shape=(1)), Input(shape=(1))\n",
    "\n",
    "embedding = Embedding(vocabulary_upper_limit, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "dot_product = dot([target, context],  axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 300)       3000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 300, 1)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 300, 1)       0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           reshape_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,000,002\n",
      "Trainable params: 3,000,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40960\n",
    "epochs = 1\n",
    "for itr in range(epochs):\n",
    "    for batch_num in range(len(word_target)//batch_size):\n",
    "        myslice = slice(batch_num*batch_size,(batch_num+1)*batch_size)\n",
    "          \n",
    "        batch_inputs = np.array(word_target[myslice]).reshape(batch_size,1)\n",
    "        batch_contexts = np.array(word_context[myslice]).reshape(batch_size,1)\n",
    "        batch_labels = np.array(labels[myslice]).reshape(batch_size,1)\n",
    "\n",
    "        loss = model.train_on_batch([batch_inputs,batch_contexts],batch_labels)\n",
    "        print(f\"Epoch: {itr}\\tBatch Num:{batch_num}/{len(word_target)//batch_size}\\tloss={loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
